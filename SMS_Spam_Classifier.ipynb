{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python : 3.5.2 (default, Nov 23 2017, 16:37:01) \n",
      "[GCC 5.4.0 20160609]\n",
      "NLTK : 3.4\n",
      "Sklearn : 0.20.0\n",
      "Pandas : 0.23.4\n",
      "Numpy : 1.15.4\n"
     ]
    }
   ],
   "source": [
    "# import all the libraries\n",
    "\n",
    "import sys\n",
    "import nltk\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "print('Python : {}'.format(sys.version))\n",
    "print('NLTK : {}'.format(nltk.__version__))\n",
    "print('Sklearn : {}'.format(sklearn.__version__))\n",
    "print('Pandas : {}'.format(pd.__version__))\n",
    "print('Numpy : {}'.format(np.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5572 entries, 0 to 5571\n",
      "Data columns (total 2 columns):\n",
      "0    5572 non-null object\n",
      "1    5572 non-null object\n",
      "dtypes: object(2)\n",
      "memory usage: 87.1+ KB\n",
      "None\n",
      "      0                                                  1\n",
      "0   ham  Go until jurong point, crazy.. Available only ...\n",
      "1   ham                      Ok lar... Joking wif u oni...\n",
      "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
      "3   ham  U dun say so early hor... U c already then say...\n",
      "4   ham  Nah I don't think he goes to usf, he lives aro...\n"
     ]
    }
   ],
   "source": [
    "# load the dataset\n",
    "\n",
    "dataset = pd.read_table('SMSSpamCollection', header = None, encoding='utf-8')\n",
    "\n",
    "# print information about the dataset\n",
    "\n",
    "print(dataset.info())\n",
    "print(dataset.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ham     4825\n",
      "spam     747\n",
      "Name: 0, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# separate the classes value(ham/spam) \n",
    "\n",
    "classes = dataset[0]\n",
    "print(classes.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert classes labels into binary values\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "Y = encoder.fit_transform(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the SMS message data\n",
    "\n",
    "X = dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use regular expressions to replace email, urls, phone numbers, other numbers, symbols, etc\n",
    "\n",
    "# replace email addresses with 'emailaddr'\n",
    "processed = X.str.replace(r'^([0-9a-zA-Z]([-.\\w]*[0-9a-zA-Z])*@(([0-9a-zA-Z])+([-\\w]*[0-9a-zA-Z])*\\.)+[a-zA-Z]{2,9})$', 'emailaddr')\n",
    "\n",
    "# replace web urls with 'webaddr'\n",
    "processed = processed.str.replace(r'^((((H|h)(T|t)|(F|f))(T|t)(P|p)((S|s)?))\\://)?(www.|[a-zA-Z0-9].)[a-zA-Z0-9\\-\\.]+\\.[a-zA-Z]{2,6}(\\:[0-9]{1,5})*(/($|[a-zA-Z0-9\\.\\,\\;\\?\\'\\\\\\+&amp;%\\$#\\=~_\\-]+))*$', 'webaddr')\n",
    "\n",
    "# replace money symbols with 'moneysymb'\n",
    "processed = processed.str.replace(r'Â£|\\$', 'moneysymb')\n",
    "\n",
    "# replace 10 digit numbers with 'phoneno'\n",
    "processed = processed.str.replace(r'^\\D?(\\d{3})\\D?\\D?(\\d{3})\\D?(\\d{4})$', 'phoneno')\n",
    "\n",
    "# replace numbers with 'number'\n",
    "processed = processed.str.replace(r'\\d+(\\.\\d+)?', 'number')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove punctuations\n",
    "processed = processed.str.replace(r'[^\\w\\d\\s]', '')\n",
    "\n",
    "# replace white spaces in between words with single spaces\n",
    "processed = processed.str.replace(r'\\s+', ' ')\n",
    "\n",
    "# remove leading and trailing whitespaces\n",
    "processed = processed.str.replace(r'^\\s+|\\s+?$', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change words to lowercase\n",
    "processed = processed.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stop words\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "processed = processed.apply(lambda x: ' '.join(term for term in x.split() if term not in stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove word stems\n",
    "\n",
    "ps = nltk.PorterStemmer()\n",
    "processed = processed.apply(lambda x: ' '.join(ps.stem(term) for term in x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize each word\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "all_words = []\n",
    "for message in processed:\n",
    "    words  =word_tokenize(message)\n",
    "    for w in words:\n",
    "        all_words.append(w)\n",
    "\n",
    "all_words = nltk.FreqDist(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words : 7392\n",
      "15 most common words : [('number', 2359), ('u', 1132), ('call', 655), ('im', 474), ('go', 452), ('get', 447), ('ur', 390), ('come', 300), ('dont', 298), ('ok', 278), ('ltgt', 276), ('free', 275), ('know', 270), ('moneysymbnumb', 260), ('like', 257)]\n"
     ]
    }
   ],
   "source": [
    "# print total words\n",
    "print('Total words : {}'.format(len(all_words)))\n",
    "\n",
    "# print 15 most common words\n",
    "print('15 most common words : {}'.format(all_words.most_common(15)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use 1500 most common words as features\n",
    "word_features = list(all_words.keys())[:1500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define find_features function\n",
    "\n",
    "def find_features(message):\n",
    "    words = word_tokenize(message)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features[word] = (word in words)\n",
    "        \n",
    "    return features\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find features of all messages\n",
    "\n",
    "messages = list(zip(processed, Y))\n",
    "\n",
    "# define seed\n",
    "seed = 1\n",
    "np.random.seed = seed\n",
    "\n",
    "# shuffle the dataset\n",
    "np.random.shuffle(messages)\n",
    "\n",
    "# call find_features function for every message in messages\n",
    "featureset = [(find_features(text), label) for (text, label) in messages]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split training and testing dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "training, testing = train_test_split(featureset, test_size = 0.25, random_state = seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scikit Learn Classifiers with NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sklearn classifiers\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define models\n",
    "\n",
    "names = ['K Neighbours', 'Decision Tree', 'Random Forest', 'Logistic Regression', 'SGD', 'Naive Bayes']\n",
    "\n",
    "classifiers = [\n",
    "    KNeighborsClassifier(),\n",
    "    DecisionTreeClassifier(),\n",
    "    RandomForestClassifier(),\n",
    "    LogisticRegression(),\n",
    "    SGDClassifier(max_iter = 100),\n",
    "    MultinomialNB(),\n",
    "    SVC(kernel = 'linear')\n",
    "]\n",
    "\n",
    "models = list(zip(names, classifiers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K Neighbours accuracy : 92.60588657573582\n",
      "Decision Tree accuracy : 94.54414931801867\n",
      "Random Forest accuracy : 94.83129935391243\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/siddhesh/680A1FF70A1FC14C/Sid/Development/Text_Classification/venv/lib/python3.5/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression accuracy : 93.82627422828428\n",
      "SGD accuracy : 94.54414931801867\n",
      "Naive Bayes accuracy : 94.61593682699211\n"
     ]
    }
   ],
   "source": [
    "# wrap models with nltk\n",
    "\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "\n",
    "for name, model in models:\n",
    "    nltk_model = SklearnClassifier(model)\n",
    "    nltk_model.train(training)\n",
    "    accuracy = nltk.classify.accuracy(nltk_model, testing) * 100\n",
    "    print('{} accuracy : {}'.format(name, accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK Ensemble accuracy : 94.54414931801867\n"
     ]
    }
   ],
   "source": [
    "# ensemble method - Voter classifier\n",
    "\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "names = ['K Neighbours', 'Decision Tree', 'Random Forest', 'Logistic Regression', 'SGD', 'Naive Bayes']\n",
    "\n",
    "classifiers = [\n",
    "    KNeighborsClassifier(),\n",
    "    DecisionTreeClassifier(),\n",
    "    RandomForestClassifier(),\n",
    "    LogisticRegression(),\n",
    "    SGDClassifier(max_iter = 100),\n",
    "    MultinomialNB(),\n",
    "    SVC(kernel = 'linear')\n",
    "]\n",
    "\n",
    "models = list(zip(names, classifiers))\n",
    "\n",
    "nltk_ensemble = SklearnClassifier(VotingClassifier(estimators = models, voting = 'hard', n_jobs = -1))\n",
    "nltk_ensemble.train(training)\n",
    "accuracy = nltk.classify.accuracy(nltk_ensemble, testing) * 100\n",
    "print('NLTK Ensemble accuracy : {}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make class label prediction fro==or testing sets\n",
    "\n",
    "txt_features, labels = zip(*testing)\n",
    "\n",
    "prediction = nltk_ensemble.classify_many(txt_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      1.00      0.97      1200\n",
      "           1       0.96      0.63      0.76       193\n",
      "\n",
      "   micro avg       0.95      0.95      0.95      1393\n",
      "   macro avg       0.95      0.81      0.87      1393\n",
      "weighted avg       0.95      0.95      0.94      1393\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th colspan=\"2\" halign=\"left\">predicted</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>ham</th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">actual</th>\n",
       "      <th>ham</th>\n",
       "      <td>1195</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spam</th>\n",
       "      <td>71</td>\n",
       "      <td>122</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            predicted     \n",
       "                  ham spam\n",
       "actual ham       1195    5\n",
       "       spam        71  122"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print classification report and confusion matrix\n",
    "\n",
    "print(classification_report(labels, prediction))\n",
    "pd.DataFrame(\n",
    "    confusion_matrix(labels, prediction),\n",
    "    index = [['actual','actual'],['ham','spam']],\n",
    "    columns = [['predicted','predicted'],['ham','spam']]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
